=====================================================
Thu Mar 21 10:05:40 EDT 2019
--ps_hosts=b16n07:2220
--worker_hosts=b16n07:2221,b16n07:2222,b16n07:2223,b16n07:2224
Parameter setup time: 0.001
Benchmark construction time: 3.287
TensorFlow:  1.13
Model:       resnet50
Dataset:     imagenet (synthetic)
Mode:        training
SingleSess:  False
Batch size:  256 global
             64 per device
Num batches: 100
Num epochs:  0.02
Devices:     ['/job:worker/replica:0/task:2/gpu:0']
NUMA bind:   False
Data format: NCHW
Optimizer:   sgd
Variables:   distributed_replicated
Sync:        True
==========
Generating training model
Initializing graph
Running warm up
Done warm up
Step	Img/sec	total_loss
1	images/sec: 218.3 +/- 0.0 (jitter = 0.0)	8.209
10	images/sec: 223.2 +/- 1.4 (jitter = 4.8)	7.878
20	images/sec: 217.0 +/- 3.7 (jitter = 7.1)	7.881
30	images/sec: 218.1 +/- 2.5 (jitter = 6.1)	7.836
40	images/sec: 218.8 +/- 1.9 (jitter = 6.4)	7.953
50	images/sec: 218.4 +/- 1.7 (jitter = 5.9)	7.734
60	images/sec: 218.3 +/- 1.5 (jitter = 6.4)	8.037
70	images/sec: 218.5 +/- 1.3 (jitter = 6.0)	7.779
80	images/sec: 219.1 +/- 1.2 (jitter = 6.2)	7.914
90	images/sec: 219.0 +/- 1.1 (jitter = 6.0)	8.046
100	images/sec: 219.0 +/- 1.0 (jitter = 6.1)	7.971
----------------------------------------------------------------
total images/sec: 875.58
----------------------------------------------------------------
Benchmark run time: 84.414
Parameter setup time: 0.001
Benchmark construction time: 3.171
TensorFlow:  1.13
Model:       resnet50
Dataset:     imagenet (synthetic)
Mode:        training
SingleSess:  False
Batch size:  256 global
             64 per device
Num batches: 100
Num epochs:  0.02
Devices:     ['/job:worker/replica:0/task:3/gpu:0']
NUMA bind:   False
Data format: NCHW
Optimizer:   sgd
Variables:   distributed_replicated
Sync:        True
==========
Generating training model
Initializing graph
Running warm up
Done warm up
Step	Img/sec	total_loss
1	images/sec: 218.3 +/- 0.0 (jitter = 0.0)	8.209
10	images/sec: 223.2 +/- 1.3 (jitter = 4.8)	7.878
20	images/sec: 217.0 +/- 3.7 (jitter = 7.1)	7.881
30	images/sec: 218.1 +/- 2.5 (jitter = 5.9)	7.836
40	images/sec: 218.8 +/- 1.9 (jitter = 6.5)	7.953
50	images/sec: 218.4 +/- 1.7 (jitter = 5.9)	7.734
60	images/sec: 218.3 +/- 1.5 (jitter = 6.3)	8.037
70	images/sec: 218.5 +/- 1.3 (jitter = 6.0)	7.779
80	images/sec: 219.1 +/- 1.2 (jitter = 6.3)	7.914
90	images/sec: 219.0 +/- 1.1 (jitter = 6.1)	8.046
100	images/sec: 219.0 +/- 1.0 (jitter = 6.2)	7.971
----------------------------------------------------------------
total images/sec: 875.58
----------------------------------------------------------------
Benchmark run time: 84.390
Parameter setup time: 0.001
Benchmark construction time: 3.289
TensorFlow:  1.13
Model:       resnet50
Dataset:     imagenet (synthetic)
Mode:        training
SingleSess:  False
Batch size:  256 global
             64 per device
Num batches: 100
Num epochs:  0.02
Devices:     ['/job:worker/replica:0/task:1/gpu:0']
NUMA bind:   False
Data format: NCHW
Optimizer:   sgd
Variables:   distributed_replicated
Sync:        True
==========
Generating training model
Initializing graph
Running warm up
Done warm up
Step	Img/sec	total_loss
1	images/sec: 218.3 +/- 0.0 (jitter = 0.0)	8.209
10	images/sec: 223.2 +/- 1.4 (jitter = 4.8)	7.878
20	images/sec: 217.0 +/- 3.7 (jitter = 7.1)	7.881
30	images/sec: 218.1 +/- 2.5 (jitter = 6.1)	7.836
40	images/sec: 218.8 +/- 1.9 (jitter = 6.5)	7.953
50	images/sec: 218.4 +/- 1.7 (jitter = 6.0)	7.734
60	images/sec: 218.3 +/- 1.5 (jitter = 6.3)	8.037
70	images/sec: 218.5 +/- 1.3 (jitter = 6.0)	7.779
80	images/sec: 219.1 +/- 1.2 (jitter = 6.3)	7.914
90	images/sec: 219.0 +/- 1.1 (jitter = 6.0)	8.046
100	images/sec: 219.0 +/- 1.0 (jitter = 6.2)	7.971
----------------------------------------------------------------
total images/sec: 875.58
----------------------------------------------------------------
Benchmark run time: 84.428
Parameter setup time: 0.001
Benchmark construction time: 3.282
TensorFlow:  1.13
Model:       resnet50
Dataset:     imagenet (synthetic)
Mode:        training
SingleSess:  False
Batch size:  256 global
             64 per device
Num batches: 100
Num epochs:  0.02
Devices:     ['/job:worker/replica:0/task:0/gpu:0']
NUMA bind:   False
Data format: NCHW
Optimizer:   sgd
Variables:   distributed_replicated
Sync:        True
==========
Generating training model
Initializing graph
Running warm up
Done warm up
Step	Img/sec	total_loss
1	images/sec: 218.3 +/- 0.0 (jitter = 0.0)	8.209
10	images/sec: 223.2 +/- 1.4 (jitter = 4.8)	7.878
20	images/sec: 217.0 +/- 3.7 (jitter = 7.1)	7.881
30	images/sec: 218.1 +/- 2.5 (jitter = 6.0)	7.836
40	images/sec: 218.8 +/- 1.9 (jitter = 6.5)	7.953
50	images/sec: 218.4 +/- 1.7 (jitter = 5.9)	7.734
60	images/sec: 218.3 +/- 1.5 (jitter = 6.4)	8.037
70	images/sec: 218.5 +/- 1.3 (jitter = 6.0)	7.779
80	images/sec: 219.1 +/- 1.2 (jitter = 6.2)	7.914
90	images/sec: 219.0 +/- 1.1 (jitter = 6.0)	8.046
100	images/sec: 219.0 +/- 1.0 (jitter = 6.2)	7.971
----------------------------------------------------------------
total images/sec: 875.58
----------------------------------------------------------------
Benchmark run time: 84.512

------------------------------------------------------------
Sender: LSF System <lsfadmin@batch2>
Subject: Job 304291: <n1_dr_4w_g1_e100> in cluster <summit> Exited

Job <n1_dr_4w_g1_e100> was submitted from host <login1> by user <jw447> in cluster <summit> at Thu Mar 21 01:45:33 2019
Job was executed on host(s) <1*batch2>, in queue <batch>, as user <jw447> in cluster <summit> at Thu Mar 21 10:05:34 2019
                            <42*b16n07>
</ccs/home/jw447> was used as the home directory.
</gpfs/alpine/proj-shared/csc143/jwang/benchmarks/scripts/tf_cnn_benchmarks/run_summit> was used as the working directory.
Started at Thu Mar 21 10:05:34 2019
Terminated at Thu Mar 21 10:35:43 2019
Results reported at Thu Mar 21 10:35:43 2019

The output (if any) is above this job summary.



PS:

Read file <n1_dr_4w_g1_e100.e> for stderr output of this job.

